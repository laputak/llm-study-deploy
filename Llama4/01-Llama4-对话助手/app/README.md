# Llama-4-Scout 对话助手

这是一个基于Llama-4-Scout-17B-16E-Instruct模型的前后端分离应用，提供了一个简单的网页界面来与模型进行对话。

## 特点

- 应用启动时预加载模型，避免每次请求都重新加载
- 模型加载状态显示，用户可以直观地了解模型加载进度
- 支持多轮对话，自动保存对话历史记录
- 可以清除对话历史，开始新的对话
- 可通过滑动条实时调整生成文本长度和历史记录长度
- 简洁美观的聊天界面
- 响应式设计，适配不同尺寸的屏幕

## 安装依赖

```bash
pip install -r requirements.txt
```

## 运行应用

```bash
python app.py
```

应用将在 http://localhost:5000 上运行。

**注意**：启动后模型会在后台自动加载，这可能需要1-2分钟。在此期间，界面会显示"模型正在加载中"的提示，加载完成后才能开始对话。

## 使用方法

1. 在浏览器中打开 http://localhost:5000
2. 等待模型加载完成（顶部的橙色通知条消失）
3. 根据需要调整参数滑动条：
   - **生成长度上限**：控制每次回复生成的最大token数（范围：256-2048）
   - **历史记录长度**：控制对话中保留的最大轮数（范围：2-20）
4. 在输入框中输入您的问题
5. 点击"发送"按钮或按Enter键发送问题
6. 等待模型生成回复
7. 继续进行多轮对话，模型会记住之前的对话内容
8. 如需清除对话历史，点击"清除对话历史"按钮

## 调整模型参数

应用提供了实时调整模型参数的功能：

- **生成长度上限 (max_new_tokens)**：控制模型每次生成的最大token数。较大的值可以让模型生成更长的回复，但会增加生成时间和资源消耗。
- **历史记录长度 (MAX_HISTORY_LENGTH)**：控制在对话中保留的最大轮数。较大的值可以让模型记住更多的上下文，但会增加输入长度和资源消耗。

通过滑动条调整的参数会立即生效，并应用于下一次对话中。

## 技术实现

- 使用Flask作为Web框架
- 应用启动时在单独的线程中加载模型，避免阻塞主线程
- 使用全局变量存储预加载的模型和tokenizer
- 使用服务器端会话管理对话历史记录
- 每个会话分配唯一的UUID，支持多用户同时使用
- 对话历史保留最近的多轮对话，防止上下文过长
- 每次请求后清理GPU缓存，减少内存占用
- 前端通过定期轮询检查模型加载状态
- 使用HTML滑动条组件实时调整模型参数

## 注意事项

- 确保服务器有足够的GPU显存来运行该模型
- 默认生成限制为1024个token，可以通过界面滑动条调整（范围：256-2048）
- 默认保留最近10轮对话历史，可以通过界面滑动条调整（范围：2-20）
- 启用了torch.no_grad()以减少内存使用
- 在生产环境中，建议使用Gunicorn或uWSGI等WSGI服务器部署 